## ğŸ¯ **Project Overview**

This repository implements a comprehensive pneumonia detection system comparing two state-of-the-art deep learning approaches: **Convolutional Neural Networks (CNNs)** and **Swin Transformer architecture**. The project provides a thorough comparative analysis of traditional computer vision methods versus modern transformer-based approaches for medical imaging classification.

### **Dual-Model Architecture Comparison**

**ğŸ—ï¸ CNN Approach:**
- **Flexible CNN Architecture**: Configurable depth, kernels, and activation functions
- **Architecture Search**: Automated hyperparameter optimization with K-Fold cross-validation
- **Medical-Focused Training**: Pneumonia recall prioritization with hierarchical early stopping
- **Robust Evaluation**: Comprehensive metrics including ROC-AUC and precision-recall curves

**ğŸ§  Swin Transformer Approach:**
- **Hierarchical Attention**: Window-based approach for better medical feature extraction  
- **Advanced Augmentation**: Focal Loss and CutMix for class imbalance mitigation
- **Cross-Validation Framework**: 5-fold CV to overcome validation instability
- **Clinical-Grade Evaluation**: Medical metrics with sensitivity/specificity focus

### **Key Innovations & Solutions**
- **Class Imbalance Combat**: Multiple strategies targeting 74% vs 26% pneumonia/normal distribution
- **Architecture Optimization**: Automated CNN architecture search with 15+ trials
- **Medical Validation Strategy**: Pneumonia recall prioritization for clinical safety
- **Comprehensive Comparison**: Head-to-head performance analysis of CNN vs Transformer approaches

---

## ğŸ§  **Model Architectures & Strategic Advantages**

### **CNN Architecture: FlexibleCNN with Automated Search**

**ğŸ—ï¸ CNN Design Philosophy:**
The CNN approach implements a highly configurable architecture that can adapt to medical imaging requirements through automated hyperparameter optimization.

**Technical Specifications:**
- **Input Resolution**: 224Ã—224 pixels (3-channel RGB)
- **Configurable Depth**: 4-6 convolutional blocks
- **Channel Architecture**: Base channels (16-48) with exponential growth (1.6x-2.0x multiplier)
- **Activation Functions**: ReLU, GELU, Swish with optional Batch Normalization
- **Pooling Strategy**: Max/Average pooling with adaptive global pooling
- **Regularization**: Dropout (0.2-0.4) and weight decay optimization
- **Parameters**: 100K-35M (architecture-dependent)

**ğŸ” CNN Architecture Search Results:**
```python
Best CNN Configuration (3-Fold CV):
â”œâ”€â”€ Depth: 6 blocks
â”œâ”€â”€ Base Channels: 48
â”œâ”€â”€ Channel Multiplier: 1.96x
â”œâ”€â”€ Kernel Size: 5x5
â”œâ”€â”€ Activation: Swish
â”œâ”€â”€ Pooling: Max (2x2)
â”œâ”€â”€ Global Pooling: Adaptive Max
â”œâ”€â”€ Batch Normalization: Disabled
â”œâ”€â”€ Dropout: 0.285
â”œâ”€â”€ Classifier: 512 hidden units
â””â”€â”€ Parameters: 33.86M
```

**Why CNN for Medical Imaging?**
| Feature | Traditional CNN | FlexibleCNN Approach | Medical Advantage |
|---------|-----------------|---------------------|-------------------|
| **Architecture** | Fixed design | Automated search | Optimal for chest X-ray patterns |
| **Feature Extraction** | Hand-crafted layers | Search-optimized blocks | Better pneumonia detection |
| **Computational Efficiency** | Standard convolution | Optimized depth/width | Efficient clinical deployment |
| **Medical Focus** | General features | Pneumonia recall prioritized | Clinical safety emphasis |

### **Swin Transformer Base (swin_base_patch4_window7_224)**

**Why Swin Over ViT for Medical Imaging?**

| Feature | Traditional ViT | Swin Transformer v2 | Medical Advantage |
|---------|-----------------|---------------------|-------------------|
| **Attention Mechanism** | Global Self-Attention | Window-based + Shifted Windows | Better locality for anatomical structures |
| **Feature Extraction** | Single-scale patches | Hierarchical multi-scale | Captures both fine details and global context |
| **Computational Efficiency** | O(nÂ²) complexity | O(n) linear complexity | Faster inference for clinical deployment |
| **Medical Relevance** | Treats all patches equally | Adaptive attention windows | Focuses on pathological regions |

**Technical Specifications:**
- **Input Resolution**: 224Ã—224 pixels
- **Patch Size**: 4Ã—4 (higher resolution than ViT's 16Ã—16)
- **Window Size**: 7Ã—7 with shifted window strategy
- **Feature Dimensions**: 1024 (Swin-B)
- **Parameters**: ~88M (optimized for M1 Max)

---

## ğŸ“Š **Enhanced Dataset Processing**

### **Class Imbalance Strategy**
```python
# Strategic Response to Dataset Imbalance
NORMAL: 1,583 samples (26.1%)
PNEUMONIA: 4,280 samples (73.9%)
Imbalance Ratio: 2.70:1
```

**V2 Enhancements:**
- **Focal Loss**: Î±=0.6, Î³=1.2 (tuned for medical context)
- **CutMix Augmentation**: 50% probability during training
- **Enhanced Transforms**: More aggressive augmentation to combat overfitting

---

## ğŸ”¬ **Advanced Training Pipeline**

### **Phase I: Cross-Validation Framework**
```python
5-Fold Cross-Validation Strategy:
â”œâ”€â”€ Fold 1-5: Individual model training with early stopping
â”œâ”€â”€ CV Performance: 94.73 Â± 1.18% (robust estimation)
â”œâ”€â”€ Early Stopping: 5/5 folds (prevents overfitting)
â””â”€â”€ Epoch Guidance: CV-guided final model selection
```

### **Phase II: Final Model Training**
- **CV-Guided Early Stopping**: Uses cross-validation insights for optimal epoch selection
- **Intelligent Model Selection**: Prevents validation overfitting through CV guidance
- **Enhanced Monitoring**: TensorBoard integration with clinical metrics

**Phase III: Clinical-Grade Evaluation**
- **Medical Metrics**: Sensitivity, Specificity, PPV, NPV
- **Performance Visualization**: ROC curves, Precision-Recall, Enhanced confusion matrices
- **Statistical Rigor**: Comprehensive per-class analysis

---

## ğŸš€ **Key Technical Innovations**

### **CNN Innovations**

**1. Flexible Architecture Design**
```python
class FlexibleCNN(nn.Module):
    """
    Configurable CNN with automated architecture search
    - Depth: 4-6 convolutional blocks
    - Channels: 16-48 base with exponential growth
    - Activations: ReLU, GELU, Swish options
    - Pooling: Max/Average with global pooling
    """
```

**2. Medical-Focused Early Stopping**
```python
class EarlyStopping:
    """
    Hierarchical early stopping for medical applications
    Primary: Pneumonia recall (sensitivity)
    Secondary: F1-score for overall balance
    """
```

**3. Architecture Search Framework**
```python
def architecture_search_cv():
    """
    3-fold CV architecture search with 15 trials
    Medical-focused selection criteria
    Automated hyperparameter optimization
    """
```

### **Swin Transformer Innovations**

**1. Focal Loss Implementation**
```python
class FocalLoss(nn.Module):
    """
    Strategic Response: Combat 74% vs 26% class imbalance
    Î± = 0.6 (favors minority normal class)
    Î³ = 1.2 (focuses on hard examples)
    """
```

**2. CutMix Augmentation**
```python
def _apply_cutmix(self, image1, label1, image2, label2):
    """
    Advanced augmentation preventing overfitting
    Creates mixed images with soft labels
    Enhances model generalization
    """
```

**3. Cross-Validation Framework**
```python
def run_cross_validation(self, train_dataset):
    """
    5-fold CV overcoming 16-sample validation instability
    Provides robust performance estimation
    Guides final model selection
    """
```

---

## ğŸ“ˆ **Performance Results**

### **CNN Performance Results**

**Architecture Search Performance:**
```
ğŸ” CNN Architecture Search (15 trials, 3-fold CV):
â”œâ”€â”€ Best Trial: 96.16 Â± 1.45% Pneumonia Recall, 91.62 Â± 0.83% F1
â”œâ”€â”€ Optimal Architecture: 6 blocks, GELU, 6.1M parameters
â”œâ”€â”€ Search Strategy: Medical-focused (pneumonia recall priority)
â””â”€â”€ Convergence: Early stopping at 4-8 epochs per fold
```

**Final CNN Test Performance:**
```
ğŸ¯ Test Accuracy: 86.06%
ğŸ” AUC-ROC: 0.9774
âš•ï¸ Pneumonia Sensitivity: 99.49%
âš•ï¸ Normal Sensitivity: 63.68%
ğŸ’Š Pneumonia PPV: 82.03%
ğŸ’Š Normal PPV: 98.68%
ğŸ“Š F1-Score: 88.26%
```

### **Swin Transformer Performance**

**Cross-Validation Performance:**
```
ğŸ“ˆ CV Performance: 94.73 Â± 1.18%
ğŸ“Š Individual Folds:
   Fold 1: 94.83% (5 epochs, Early Stop)
   Fold 2: 92.43% (6 epochs, Early Stop)
   Fold 3: 95.59% (6 epochs, Early Stop)
   Fold 4: 95.30% (2 epochs, Early Stop)
   Fold 5: 95.49% (6 epochs, Early Stop)
```

**Final Swin Test Performance:**
```
ğŸ¯ Test Accuracy: 86.06%
ğŸ” AUC-ROC: 0.9774
âš•ï¸ Pneumonia Sensitivity: 99.49%
âš•ï¸ Normal Sensitivity: 63.68%
ğŸ’Š Pneumonia PPV: 82.03%
ğŸ’Š Normal PPV: 98.68%
```

### **Clinical Interpretation**
- **CNN Advantages**: More efficient, automated optimization, slightly better pneumonia detection
- **Swin Advantages**: Comprehensive validation, attention mechanisms for interpretability
- **Both Models**: Excellent pneumonia sensitivity (>94%), suitable for clinical screening

---

## ğŸ›  **Installation & Setup**

### **Prerequisites**
- macOS with Apple Silicon (M1/M1 Pro/M1 Max/M2)
- Python 3.8+
- PyTorch with MPS support
- 8GB+ RAM recommended

### **Quick Installation**
```bash
# Clone repository
git clone <repository-url>
cd pneumonia-swin-transformer-v2

# Install dependencies
pip install torch torchvision torchaudio
pip install timm albumentations opencv-python
pip install scikit-learn matplotlib seaborn
pip install tensorboard tqdm

# Verify MPS availability
python -c "import torch; print(f'MPS Available: {torch.backends.mps.is_available()}')"
```

### **Dataset Setup**
```bash
# Expected directory structure
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ NORMAL/     # Normal chest X-rays
â”‚   â””â”€â”€ PNEUMONIA/  # Pneumonia chest X-rays
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ NORMAL/
â”‚   â””â”€â”€ PNEUMONIA/
â””â”€â”€ test/
    â”œâ”€â”€ NORMAL/
    â””â”€â”€ PNEUMONIA/
```

---

## ğŸ¯ **Training Configuration**

### **Model Configuration**
```python
@dataclass
class ModelConfigV2:
    # Architecture
    model_name: str = "swin_base_patch4_window7_224"
    batch_size: int = 32
    num_epochs: int = 10
    learning_rate: float = 2e-4
    
    # Class Imbalance Combat
    focal_alpha: float = 0.6
    focal_gamma: float = 1.2
    
    # Augmentation Strategy
    cutmix_prob: float = 0.5
    cutmix_alpha: float = 1.0
    
    # Cross-Validation
    n_folds: int = 5
    cv_random_seed: int = 42
```

### **Training Execution**
```bash
# Full pipeline execution
python FinalProject_Q2_swin_transformer.py

# Expected output:
ğŸ¥ SOTA Pneumonia Detection v2 - Swin Transformer
======================================================================
ğŸ–¥ï¸  Device: mps
ğŸ§  Model: swin_base_patch4_window7_224
ğŸ“Š Image Size: 224x224
ğŸ”„ Epochs: 10
ğŸ¯ Focal Loss: Î±=0.6, Î³=1.2
ğŸ”€ CutMix: 50% probability
ğŸ“ˆ Cross-Validation: 5 folds
======================================================================
```

---

## ğŸ“Š **Monitoring & Analysis**

### **TensorBoard Integration**
```bash
# Launch TensorBoard for training monitoring
tensorboard --logdir=./logs_v2

# Available metrics:
â”œâ”€â”€ Cross-validation folds (fold_0 to fold_4)
â”œâ”€â”€ Final model training
â”œâ”€â”€ Loss curves (train/validation)
â”œâ”€â”€ Accuracy trends
â””â”€â”€ Learning rate scheduling
```

### **Output Analysis**
```bash
outputs/
â”œâ”€â”€ cross_validation_results.json     # CV performance metrics
â”œâ”€â”€ test_evaluation_metrics.json   # Final test results
â”œâ”€â”€ enhanced_confusion_matrix.png  # Clinical confusion matrix
â”œâ”€â”€ roc_curve.png                  # ROC analysis
â””â”€â”€ precision_recall_curve.png     # PR curve analysis
```

---

## ğŸ— **Project Structure**

```
ğŸ“¦ Advanced Pneumonia Detection System
â”œâ”€â”€ ğŸ“œ CNN_CV.ipynb                         # CNN implementation with architecture search
â”œâ”€â”€ ğŸ“œ train_swin_transformer.py            # Swin Transformer implementation
â”œâ”€â”€ ğŸ“œ README.md                            # This comprehensive guide
â”œâ”€â”€ ğŸ“‚ data/                                # Dataset directory
â”‚   â”œâ”€â”€ ğŸ“‚ train/                           # Training images (5,216 samples)
â”‚   â”œâ”€â”€ ğŸ“‚ val/                             # Validation images (16 samples)
â”‚   â””â”€â”€ ğŸ“‚ test/                            # Test images (624 samples)
â”œâ”€â”€ ğŸ“‚ outputs/                             # Results & visualizations
â”‚   â”œâ”€â”€ ğŸ“Š CNN_Cross_Validation_Results.csv # CNN architecture search results
â”‚   â”œâ”€â”€ ğŸ“Š Swin_cross_validation_results.json # Swin CV performance metrics
â”‚   â”œâ”€â”€ ğŸ“Š Swin_test_evaluation_metrics.json # Final test results
â”‚   â”œâ”€â”€ ğŸ“ˆ CNN_ROC.png                      # CNN ROC curve analysis
â”‚   â”œâ”€â”€ ğŸ“ˆ CNN_PR.png                       # CNN Precision-Recall curve
â”‚   â”œâ”€â”€ ğŸ“ˆ CM CNN.png                       # CNN confusion matrix
â”‚   â”œâ”€â”€ ğŸ“ˆ enhanced_confusion_matrix.png    # Swin confusion matrix
â”‚   â”œâ”€â”€ ğŸ“ˆ roc_curve.png                    # Swin ROC analysis
â”‚   â””â”€â”€ ğŸ“ˆ precision_recall_curve.png       # Swin PR curve analysis
â”œâ”€â”€ ğŸ“‚ checkpoints/                         # Model checkpoints
â”‚   â”œâ”€â”€ ğŸ’¾ final_model_cnn.pt               # Best CNN model
â”‚   â””â”€â”€ ğŸ’¾ final_model_swin.pth             # Best Swin model
â””â”€â”€ ğŸ“‚ logs/                                # TensorBoard logs
    â”œâ”€â”€ ğŸ“ fold_0/ to fold_4/               # Swin CV fold logs
    â””â”€â”€ ğŸ“ final_model/                     # Final training logs
```

---

## ğŸ”¬ **Technical Implementation Details**

### **Core Components**

#### **1. CNN Implementation Components**
```python
class FlexibleCNN(nn.Module):
    """
    Configurable CNN architecture:
    - Variable depth (4-6 blocks)
    - Multiple activation functions
    - Automated architecture search
    - Medical-focused early stopping
    """

def architecture_search_cv():
    """
    3-fold cross-validation architecture search:
    - 15 random configuration trials
    - Medical hierarchy stopping criteria
    - Automated hyperparameter optimization
    """
```

#### **2. Enhanced Dataset Class**
```python
class ChestXrayDatasetV2(Dataset):
    """
    Key Features:
    - CutMix augmentation integration
    - Enhanced class distribution logging
    - Cross-validation optimization
    """
```

#### **3. Swin Transformer Architecture**
```python
class PneumoniaSwinV2(nn.Module):
    """
    Hierarchical attention for medical imaging:
    - Window-based attention mechanism
    - Multi-scale feature extraction
    - Enhanced classification head
    """
```

#### **4. Advanced Training Engine**
```python
class PneumoniaTrainerV2:
    """
    Production-grade training with:
    - 5-fold cross-validation
    - CV-guided early stopping
    - Focal loss integration
    """
```

#### **5. Clinical Evaluation Suite**
```python
class PneumoniaEvaluatorV2:
    """
    Medical-grade evaluation:
    - Clinical metrics (Sensitivity, Specificity, PPV)
    - Enhanced visualizations
    - Statistical analysis
    """
```

---

## ğŸ¨ **Advanced Augmentation Strategy**

### **Enhanced Medical Transforms**
```python
# V2 Augmentation Pipeline
A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=15, p=0.4)
A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5)
A.RandomGamma(gamma_limit=(70, 130), p=0.4)
A.GaussNoise(var_limit=(10.0, 60.0), p=0.3)
A.MotionBlur(blur_limit=5, p=0.15)
A.ElasticTransform(alpha=50, sigma=5, p=0.1)  # New for v2
```

**Strategic Justification:**
- **More Aggressive Augmentation**: Combat overfitting observed in v1
- **Medical Preservation**: Careful parameter tuning to maintain diagnostic features
- **CutMix Integration**: Advanced mixing strategy for robust feature learning

---

## ğŸ“‹ **Dependencies**

```txt
torch>=1.13.0
torchvision>=0.14.0
timm>=0.6.12
albumentations>=1.3.0
opencv-python>=4.7.0
scikit-learn>=1.2.0
matplotlib>=3.6.0
seaborn>=0.12.0
numpy>=1.24.0
tqdm>=4.64.0
tensorboard>=2.11.0
```

---

## ğŸ”§ **Configuration & Customization**

### **Hyperparameter Tuning**
```python
# Focal Loss Parameters (class imbalance)
focal_alpha: float = 0.6    # Adjust for dataset imbalance
focal_gamma: float = 1.2    # Control focus on hard examples

# CutMix Configuration (overfitting prevention)
cutmix_prob: float = 0.5    # Probability of applying CutMix
cutmix_alpha: float = 1.0   # Beta distribution parameter

# Training Parameters
learning_rate: float = 2e-4 # Conservative for Swin
weight_decay: float = 0.02  # Regularization strength
```

### **Cross-Validation Settings**
```python
n_folds: int = 5            # Number of CV folds
cv_random_seed: int = 42    # Reproducible splits
```

---

## ğŸ“ˆ **Performance Comparison**


### **Clinical Interpretation**
- **CNN Advantages**: More efficient, automated optimization, slightly better pneumonia detection
- **Swin Advantages**: Comprehensive validation, attention mechanisms for interpretability
- **Both Models**: Excellent pneumonia sensitivity (>94%), suitable for clinical screening
- **Deployment Consideration**: CNN preferred for resource-constrained environments

---

## ğŸš€ **Quick Start Guide**

### **1. CNN Training (Architecture Search + Final Model)**
```bash
# Run the CNN notebook with architecture search
jupyter notebook CNN_CV.ipynb

# Follow the notebook cells sequentially:
# - Data loading and preprocessing
# - Architecture search (15 trials, 3-fold CV)
# - Final model training with best configuration
# - Comprehensive evaluation and visualization
```

### **2. Swin Transformer Training**
```bash
# Run with default configuration
python train_swin_transformer.py

# Expected output:
ğŸ¥ SOTA Pneumonia Detection v2 - Swin Transformer
======================================================================
ğŸ–¥ï¸  Device: mps
ğŸ§  Model: swin_base_patch4_window7_224
ğŸ“Š Image Size: 224x224
ğŸ”„ Epochs: 10
ğŸ¯ Focal Loss: Î±=0.6, Î³=1.2
ğŸ”€ CutMix: 50% probability
ğŸ“ˆ Cross-Validation: 5 folds
======================================================================
```

### **3. Custom Configuration (Swin)**
```python
# Modify configuration parameters
config_v2.focal_alpha = 0.7     # Adjust class weighting
config_v2.cutmix_prob = 0.3     # Reduce augmentation
config_v2.num_epochs = 15       # Extended training
```

---

## ğŸ¥ **Clinical Integration Roadmap**

### **Phase 1: Validation (Current)**
- âœ… Cross-validation framework
- âœ… Clinical metrics implementation
- âœ… Performance visualization

### **Phase 2: Deployment Preparation**
- ğŸ”„ Model quantization for mobile deployment
- ğŸ”„ ONNX export for cross-platform compatibility
- ğŸ”„ API wrapper development

### **Phase 3: Clinical Validation**
- ğŸ“‹ Radiologist annotation comparison
- ğŸ“‹ Multi-center validation study
- ğŸ“‹ Regulatory compliance preparation

### **Phase 4: Production Deployment**
- ğŸ¯ Hospital system integration
- ğŸ¯ Real-time inference optimization
- ğŸ¯ Continuous learning pipeline

---

## ğŸ“š **Research Background**

### **Key Papers & Inspiration**
1. **Swin Transformer**: Liu et al., "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows" (ICCV 2021)
2. **Focal Loss**: Lin et al., "Focal Loss for Dense Object Detection" (ICCV 2017)
3. **CutMix**: Yun et al., "CutMix: Regularization Strategy to Train Strong Classifiers" (ICCV 2019)
4. **Medical AI**: Rajpurkar et al., "CheXNet: Radiologist-Level Pneumonia Detection" (2017)

### **Strategic Innovations**
- **Hierarchical Medical Attention**: Adapting Swin's window mechanism for anatomical structures
- **Medical Class Imbalance**: Focal Loss optimization for clinical datasets
- **Validation Instability Solution**: Cross-validation framework for small validation sets

---

## ğŸ”¬ **Future Enhancements**

### **Immediate Roadmap**
- [ ] **Multi-Class Extension**: Bacterial vs Viral pneumonia classification
- [ ] **Ensemble Methods**: Combine multiple Swin variants
- [ ] **Attention Visualization**: Medical-specific attention map generation
- [ ] **Uncertainty Quantification**: Bayesian neural networks for confidence estimation

### **Advanced Features**
- [ ] **Federated Learning**: Multi-hospital collaborative training
- [ ] **Active Learning**: Intelligent data annotation prioritization
- [ ] **Causal Analysis**: Understanding model decision pathways
- [ ] **Robustness Testing**: Adversarial attack resistance
